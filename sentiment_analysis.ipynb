{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import gensim \n",
    "import tensorflow as tf\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "We read the input IITB corpus into a list of sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration cfilt--iitb-english-hindi-930ee63dc3ad2bff\n",
      "Reusing dataset parquet (C:\\Users\\parth\\.cache\\huggingface\\datasets\\parquet\\cfilt--iitb-english-hindi-930ee63dc3ad2bff\\0.0.0\\0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
      "100%|██████████| 3/3 [00:00<00:00, 11.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "corpus_data = load_dataset('cfilt/iitb-english-hindi')\n",
    "translation_data = corpus_data[\"train\"][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [list_element['en'] for list_element in translation_data]\n",
    "iitb_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('training_data.csv')\n",
    "X_train = train_data.loc[:, 'text']\n",
    "y_train = train_data.loc[:, 'airline_sentiment']\n",
    "split_index = int(len(X_train) * 0.8)\n",
    "X_test = X_train[split_index:]\n",
    "y_test = y_train[split_index:]\n",
    "X_train = X_train[:split_index]\n",
    "y_train = y_train[:split_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we remove any unwanted details in the dataset. These details include URLs, Emails, New line characters etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depure_data(data):\n",
    "    \n",
    "    #Removing URLs with a regular expression\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub(r'', data)\n",
    "\n",
    "    # Remove Emails\n",
    "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = re.sub('\\s+', ' ', data)\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = re.sub(\"\\'\", \"\", data)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on, we remove the longest and the shortest words in the dataset since they are mostly useless<br/>\n",
    "For this, we use gensim library's simple_preprocess function which ignores tokens shorter than 2 and longer than 15 letter in length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detokenization of all sentences is needed again because we will feed a proper sentence with context to the model as the input for proper sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordDetokenizer\n",
    "\n",
    "def detokenize(text):\n",
    "    return TreebankWordDetokenizer().detokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem(words):\n",
    "    stemmed_words = []\n",
    "    for word in words: \n",
    "        stemmed_words.append(stemmer.stem(word))\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['give your applic an access workout', 'accercis access explor', 'the default plugin layout for the bottom panel', 'the default plugin layout for the top panel', 'list of plugin that are disabl by default']\n"
     ]
    }
   ],
   "source": [
    "# tokenize the sentences \n",
    "temp = []\n",
    "for sentence in data:\n",
    "    temp.append(depure_data(sentence))\n",
    "tokenized_sentences = list(sent_to_words(temp))\n",
    "\n",
    "# stem the tokens \n",
    "temp = []\n",
    "for tokenized_sentence in tokenized_sentences:\n",
    "    temp.append(stem(tokenized_sentence))\n",
    "tokenized_sentences = temp\n",
    "\n",
    "# detokenize the stemmed tokens to form sentences with context \n",
    "# context is needed for further processing \n",
    "d = []\n",
    "for data_word in tokenized_sentences: \n",
    "    d.append(detokenize(data_word))\n",
    "print(d[:5])\n",
    "\n",
    "data = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151588\n"
     ]
    }
   ],
   "source": [
    "word_set = set()\n",
    "for sentence in data: \n",
    "    sent_set = set(sentence.split())\n",
    "    word_set.update(sent_set)\n",
    "print(len(word_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have preprocessed the data, we can move onto converting them into vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1659083, 200)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_words = 200000\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "data = pad_sequences(sequences, maxlen=max_len)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for sentence in X_train:\n",
    "    temp.append(depure_data(sentence))\n",
    "tokenized_sentences = list(sent_to_words(temp))\n",
    "temp = []\n",
    "for tokenized_sentence in tokenized_sentences:\n",
    "    temp.append(stem(tokenized_sentence))\n",
    "tokenized_sentences = temp\n",
    "d = []\n",
    "for data_word in tokenized_sentences: \n",
    "    d.append(detokenize(data_word))\n",
    "X_train = d\n",
    "\n",
    "temp = []\n",
    "for sentence in X_test:\n",
    "    temp.append(depure_data(sentence))\n",
    "tokenized_sentences = list(sent_to_words(temp))\n",
    "temp = []\n",
    "for tokenized_sentence in tokenized_sentences:\n",
    "    temp.append(stem(tokenized_sentence))\n",
    "tokenized_sentences = temp\n",
    "d = []\n",
    "for data_word in tokenized_sentences: \n",
    "    d.append(detokenize(data_word))\n",
    "X_test = d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the corpus cleansed, we now proceed to the word embedding part of the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the training target label from categorical to numerical and encode them using one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for sentiment in y_train: \n",
    "    if sentiment == 'neutral':\n",
    "        temp.append(0)\n",
    "    if sentiment == 'negative':\n",
    "        temp.append(1)\n",
    "    if sentiment == 'positive':\n",
    "        temp.append(2)\n",
    "y_train = np.array(temp)\n",
    "\n",
    "\n",
    "temp = []\n",
    "for sentiment in y_test: \n",
    "    if sentiment == 'neutral':\n",
    "        temp.append(0)\n",
    "    if sentiment == 'negative':\n",
    "        temp.append(1)\n",
    "    if sentiment == 'positive':\n",
    "        temp.append(2)\n",
    "y_test = np.array(temp)\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 3, dtype=\"float32\")\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 3, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11712, 200)\n",
      "(11712, 3)\n",
      "(2928, 200)\n",
      "(2928, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 20)          4000000   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 15)                2160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 48        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,002,208\n",
      "Trainable params: 4,002,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.8006 - accuracy: 0.6502\n",
      "Epoch 1: val_accuracy improved from -inf to 0.69092, saving model to best_model1.hdf5\n",
      "366/366 [==============================] - 20s 50ms/step - loss: 0.8006 - accuracy: 0.6502 - val_loss: 0.7791 - val_accuracy: 0.6909\n",
      "Epoch 2/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.6543 - accuracy: 0.7254\n",
      "Epoch 2: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.6545 - accuracy: 0.7253 - val_loss: 0.8496 - val_accuracy: 0.6551\n",
      "Epoch 3/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.5925 - accuracy: 0.7624\n",
      "Epoch 3: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.5925 - accuracy: 0.7624 - val_loss: 0.9523 - val_accuracy: 0.6267\n",
      "Epoch 4/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.5457 - accuracy: 0.7848\n",
      "Epoch 4: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.5457 - accuracy: 0.7848 - val_loss: 0.9783 - val_accuracy: 0.6185\n",
      "Epoch 5/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.5170 - accuracy: 0.8010\n",
      "Epoch 5: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.5170 - accuracy: 0.8010 - val_loss: 0.9568 - val_accuracy: 0.6387\n",
      "Epoch 6/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.4865 - accuracy: 0.8090\n",
      "Epoch 6: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.4865 - accuracy: 0.8088 - val_loss: 1.0025 - val_accuracy: 0.6387\n",
      "Epoch 7/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.4717 - accuracy: 0.8212\n",
      "Epoch 7: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.4718 - accuracy: 0.8213 - val_loss: 0.9803 - val_accuracy: 0.6554\n",
      "Epoch 8/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.4570 - accuracy: 0.8236\n",
      "Epoch 8: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.4570 - accuracy: 0.8236 - val_loss: 1.0286 - val_accuracy: 0.6366\n",
      "Epoch 9/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.4491 - accuracy: 0.8305\n",
      "Epoch 9: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.4491 - accuracy: 0.8305 - val_loss: 1.0127 - val_accuracy: 0.6564\n",
      "Epoch 10/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.4385 - accuracy: 0.8345\n",
      "Epoch 10: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.4384 - accuracy: 0.8345 - val_loss: 1.0253 - val_accuracy: 0.6417\n",
      "Epoch 11/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.4321 - accuracy: 0.8341\n",
      "Epoch 11: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.4321 - accuracy: 0.8341 - val_loss: 1.0519 - val_accuracy: 0.6588\n",
      "Epoch 12/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.4247 - accuracy: 0.8371\n",
      "Epoch 12: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 45ms/step - loss: 0.4249 - accuracy: 0.8370 - val_loss: 1.0052 - val_accuracy: 0.6609\n",
      "Epoch 13/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.4205 - accuracy: 0.8419\n",
      "Epoch 13: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 45ms/step - loss: 0.4214 - accuracy: 0.8414 - val_loss: 1.0366 - val_accuracy: 0.6684\n",
      "Epoch 14/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.4185 - accuracy: 0.8398\n",
      "Epoch 14: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.4185 - accuracy: 0.8398 - val_loss: 1.0642 - val_accuracy: 0.6749\n",
      "Epoch 15/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.4126 - accuracy: 0.8438\n",
      "Epoch 15: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 16s 45ms/step - loss: 0.4122 - accuracy: 0.8439 - val_loss: 1.0840 - val_accuracy: 0.6441\n",
      "Epoch 16/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.4013 - accuracy: 0.8473\n",
      "Epoch 16: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 16s 45ms/step - loss: 0.4013 - accuracy: 0.8473 - val_loss: 1.0757 - val_accuracy: 0.6510\n",
      "Epoch 17/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.8502\n",
      "Epoch 17: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 16s 45ms/step - loss: 0.3998 - accuracy: 0.8502 - val_loss: 1.0690 - val_accuracy: 0.6506\n",
      "Epoch 18/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3978 - accuracy: 0.8451\n",
      "Epoch 18: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 45ms/step - loss: 0.3984 - accuracy: 0.8450 - val_loss: 1.0915 - val_accuracy: 0.6622\n",
      "Epoch 19/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3928 - accuracy: 0.8521\n",
      "Epoch 19: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 45ms/step - loss: 0.3928 - accuracy: 0.8521 - val_loss: 1.0791 - val_accuracy: 0.6537\n",
      "Epoch 20/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3938 - accuracy: 0.8494\n",
      "Epoch 20: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 16s 45ms/step - loss: 0.3935 - accuracy: 0.8496 - val_loss: 1.0812 - val_accuracy: 0.6653\n",
      "Epoch 21/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3886 - accuracy: 0.8529\n",
      "Epoch 21: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 45ms/step - loss: 0.3883 - accuracy: 0.8531 - val_loss: 1.0713 - val_accuracy: 0.6489\n",
      "Epoch 22/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3837 - accuracy: 0.8566\n",
      "Epoch 22: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 45ms/step - loss: 0.3837 - accuracy: 0.8566 - val_loss: 1.1471 - val_accuracy: 0.6301\n",
      "Epoch 23/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3814 - accuracy: 0.8589\n",
      "Epoch 23: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 45ms/step - loss: 0.3809 - accuracy: 0.8592 - val_loss: 1.0965 - val_accuracy: 0.6489\n",
      "Epoch 24/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3726 - accuracy: 0.8584\n",
      "Epoch 24: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 45ms/step - loss: 0.3726 - accuracy: 0.8584 - val_loss: 1.1024 - val_accuracy: 0.6520\n",
      "Epoch 25/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3724 - accuracy: 0.8620\n",
      "Epoch 25: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 45ms/step - loss: 0.3724 - accuracy: 0.8621 - val_loss: 1.1441 - val_accuracy: 0.6619\n",
      "Epoch 26/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3684 - accuracy: 0.8616\n",
      "Epoch 26: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3684 - accuracy: 0.8616 - val_loss: 1.1554 - val_accuracy: 0.6448\n",
      "Epoch 27/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3647 - accuracy: 0.8628\n",
      "Epoch 27: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3645 - accuracy: 0.8630 - val_loss: 1.1589 - val_accuracy: 0.6376\n",
      "Epoch 28/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3611 - accuracy: 0.8636\n",
      "Epoch 28: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3611 - accuracy: 0.8636 - val_loss: 1.1675 - val_accuracy: 0.6448\n",
      "Epoch 29/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3541 - accuracy: 0.8661\n",
      "Epoch 29: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3541 - accuracy: 0.8661 - val_loss: 1.1330 - val_accuracy: 0.6298\n",
      "Epoch 30/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3587 - accuracy: 0.8635\n",
      "Epoch 30: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3587 - accuracy: 0.8635 - val_loss: 1.1499 - val_accuracy: 0.6492\n",
      "Epoch 31/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3537 - accuracy: 0.8688\n",
      "Epoch 31: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3537 - accuracy: 0.8688 - val_loss: 1.1855 - val_accuracy: 0.6253\n",
      "Epoch 32/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3497 - accuracy: 0.8695\n",
      "Epoch 32: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3505 - accuracy: 0.8694 - val_loss: 1.1587 - val_accuracy: 0.6376\n",
      "Epoch 33/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3510 - accuracy: 0.8689\n",
      "Epoch 33: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3510 - accuracy: 0.8689 - val_loss: 1.2259 - val_accuracy: 0.6124\n",
      "Epoch 34/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3442 - accuracy: 0.8708\n",
      "Epoch 34: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3442 - accuracy: 0.8708 - val_loss: 1.1888 - val_accuracy: 0.6626\n",
      "Epoch 35/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3426 - accuracy: 0.8733\n",
      "Epoch 35: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3426 - accuracy: 0.8733 - val_loss: 1.2150 - val_accuracy: 0.6329\n",
      "Epoch 36/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3408 - accuracy: 0.8730\n",
      "Epoch 36: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3408 - accuracy: 0.8730 - val_loss: 1.1961 - val_accuracy: 0.6486\n",
      "Epoch 37/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3403 - accuracy: 0.8744\n",
      "Epoch 37: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3403 - accuracy: 0.8744 - val_loss: 1.2257 - val_accuracy: 0.6414\n",
      "Epoch 38/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3395 - accuracy: 0.8746\n",
      "Epoch 38: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3395 - accuracy: 0.8746 - val_loss: 1.2225 - val_accuracy: 0.6141\n",
      "Epoch 39/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3323 - accuracy: 0.8790\n",
      "Epoch 39: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 47ms/step - loss: 0.3323 - accuracy: 0.8790 - val_loss: 1.2388 - val_accuracy: 0.6264\n",
      "Epoch 40/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3276 - accuracy: 0.8776\n",
      "Epoch 40: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3272 - accuracy: 0.8778 - val_loss: 1.2518 - val_accuracy: 0.6158\n",
      "Epoch 41/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3303 - accuracy: 0.8781\n",
      "Epoch 41: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3303 - accuracy: 0.8781 - val_loss: 1.2796 - val_accuracy: 0.6243\n",
      "Epoch 42/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3268 - accuracy: 0.8772\n",
      "Epoch 42: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3268 - accuracy: 0.8772 - val_loss: 1.2360 - val_accuracy: 0.6230\n",
      "Epoch 43/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3273 - accuracy: 0.8795\n",
      "Epoch 43: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 47ms/step - loss: 0.3271 - accuracy: 0.8795 - val_loss: 1.2142 - val_accuracy: 0.6339\n",
      "Epoch 44/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3248 - accuracy: 0.8821\n",
      "Epoch 44: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3253 - accuracy: 0.8817 - val_loss: 1.2271 - val_accuracy: 0.6404\n",
      "Epoch 45/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3232 - accuracy: 0.8823\n",
      "Epoch 45: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3232 - accuracy: 0.8823 - val_loss: 1.2360 - val_accuracy: 0.6250\n",
      "Epoch 46/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3173 - accuracy: 0.8846\n",
      "Epoch 46: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 47ms/step - loss: 0.3174 - accuracy: 0.8844 - val_loss: 1.2525 - val_accuracy: 0.6192\n",
      "Epoch 47/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3177 - accuracy: 0.8832\n",
      "Epoch 47: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3177 - accuracy: 0.8832 - val_loss: 1.2997 - val_accuracy: 0.6182\n",
      "Epoch 48/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3174 - accuracy: 0.8847\n",
      "Epoch 48: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3178 - accuracy: 0.8846 - val_loss: 1.2307 - val_accuracy: 0.6281\n",
      "Epoch 49/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3218 - accuracy: 0.8819\n",
      "Epoch 49: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3218 - accuracy: 0.8819 - val_loss: 1.2365 - val_accuracy: 0.6335\n",
      "Epoch 50/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.8858\n",
      "Epoch 50: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3153 - accuracy: 0.8859 - val_loss: 1.2609 - val_accuracy: 0.6438\n",
      "Epoch 51/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3068 - accuracy: 0.8864\n",
      "Epoch 51: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3068 - accuracy: 0.8864 - val_loss: 1.2707 - val_accuracy: 0.6342\n",
      "Epoch 52/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3181 - accuracy: 0.8842\n",
      "Epoch 52: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3181 - accuracy: 0.8842 - val_loss: 1.2719 - val_accuracy: 0.6455\n",
      "Epoch 53/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3133 - accuracy: 0.8833\n",
      "Epoch 53: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 47ms/step - loss: 0.3135 - accuracy: 0.8832 - val_loss: 1.2249 - val_accuracy: 0.6363\n",
      "Epoch 54/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3109 - accuracy: 0.8894\n",
      "Epoch 54: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3109 - accuracy: 0.8894 - val_loss: 1.2702 - val_accuracy: 0.6373\n",
      "Epoch 55/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3087 - accuracy: 0.8872\n",
      "Epoch 55: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3085 - accuracy: 0.8872 - val_loss: 1.2743 - val_accuracy: 0.6486\n",
      "Epoch 56/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3088 - accuracy: 0.8854\n",
      "Epoch 56: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3082 - accuracy: 0.8858 - val_loss: 1.2902 - val_accuracy: 0.6342\n",
      "Epoch 57/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.8882\n",
      "Epoch 57: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3045 - accuracy: 0.8878 - val_loss: 1.2550 - val_accuracy: 0.6257\n",
      "Epoch 58/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3078 - accuracy: 0.8869\n",
      "Epoch 58: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3075 - accuracy: 0.8870 - val_loss: 1.2669 - val_accuracy: 0.6270\n",
      "Epoch 59/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.3006 - accuracy: 0.8890\n",
      "Epoch 59: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3008 - accuracy: 0.8887 - val_loss: 1.3037 - val_accuracy: 0.6455\n",
      "Epoch 60/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2982 - accuracy: 0.8897\n",
      "Epoch 60: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2984 - accuracy: 0.8894 - val_loss: 1.2697 - val_accuracy: 0.6267\n",
      "Epoch 61/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.8861\n",
      "Epoch 61: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3052 - accuracy: 0.8861 - val_loss: 1.2944 - val_accuracy: 0.6161\n",
      "Epoch 62/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2994 - accuracy: 0.8900\n",
      "Epoch 62: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2996 - accuracy: 0.8899 - val_loss: 1.3062 - val_accuracy: 0.6332\n",
      "Epoch 63/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.3013 - accuracy: 0.8904\n",
      "Epoch 63: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.3013 - accuracy: 0.8904 - val_loss: 1.2935 - val_accuracy: 0.6212\n",
      "Epoch 64/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2961 - accuracy: 0.8926\n",
      "Epoch 64: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2961 - accuracy: 0.8926 - val_loss: 1.3201 - val_accuracy: 0.6175\n",
      "Epoch 65/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2988 - accuracy: 0.8905\n",
      "Epoch 65: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 47ms/step - loss: 0.2988 - accuracy: 0.8905 - val_loss: 1.3195 - val_accuracy: 0.6260\n",
      "Epoch 66/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2958 - accuracy: 0.8929\n",
      "Epoch 66: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 47ms/step - loss: 0.2958 - accuracy: 0.8929 - val_loss: 1.2841 - val_accuracy: 0.6332\n",
      "Epoch 67/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2961 - accuracy: 0.8940\n",
      "Epoch 67: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2961 - accuracy: 0.8940 - val_loss: 1.2808 - val_accuracy: 0.6335\n",
      "Epoch 68/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.8932\n",
      "Epoch 68: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2915 - accuracy: 0.8931 - val_loss: 1.2805 - val_accuracy: 0.6329\n",
      "Epoch 69/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.8905\n",
      "Epoch 69: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2952 - accuracy: 0.8905 - val_loss: 1.3219 - val_accuracy: 0.6178\n",
      "Epoch 70/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8908\n",
      "Epoch 70: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2941 - accuracy: 0.8909 - val_loss: 1.3862 - val_accuracy: 0.6253\n",
      "Epoch 71/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2915 - accuracy: 0.8959\n",
      "Epoch 71: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2915 - accuracy: 0.8959 - val_loss: 1.3038 - val_accuracy: 0.6274\n",
      "Epoch 72/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2916 - accuracy: 0.8933\n",
      "Epoch 72: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2916 - accuracy: 0.8933 - val_loss: 1.3343 - val_accuracy: 0.6335\n",
      "Epoch 73/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2927 - accuracy: 0.8930\n",
      "Epoch 73: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2927 - accuracy: 0.8930 - val_loss: 1.3333 - val_accuracy: 0.6370\n",
      "Epoch 74/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2912 - accuracy: 0.8952\n",
      "Epoch 74: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2912 - accuracy: 0.8952 - val_loss: 1.2899 - val_accuracy: 0.6120\n",
      "Epoch 75/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.8943\n",
      "Epoch 75: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2877 - accuracy: 0.8946 - val_loss: 1.3612 - val_accuracy: 0.6113\n",
      "Epoch 76/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2847 - accuracy: 0.8957\n",
      "Epoch 76: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2847 - accuracy: 0.8957 - val_loss: 1.3468 - val_accuracy: 0.6349\n",
      "Epoch 77/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2860 - accuracy: 0.8969\n",
      "Epoch 77: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2860 - accuracy: 0.8969 - val_loss: 1.3683 - val_accuracy: 0.5977\n",
      "Epoch 78/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2840 - accuracy: 0.8990\n",
      "Epoch 78: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2841 - accuracy: 0.8989 - val_loss: 1.3202 - val_accuracy: 0.6356\n",
      "Epoch 79/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2848 - accuracy: 0.8961\n",
      "Epoch 79: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 47ms/step - loss: 0.2848 - accuracy: 0.8962 - val_loss: 1.3133 - val_accuracy: 0.6397\n",
      "Epoch 80/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2791 - accuracy: 0.8963\n",
      "Epoch 80: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 18s 48ms/step - loss: 0.2791 - accuracy: 0.8963 - val_loss: 1.3144 - val_accuracy: 0.6407\n",
      "Epoch 81/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2790 - accuracy: 0.9009\n",
      "Epoch 81: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2786 - accuracy: 0.9011 - val_loss: 1.3240 - val_accuracy: 0.6301\n",
      "Epoch 82/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2803 - accuracy: 0.9002\n",
      "Epoch 82: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 47ms/step - loss: 0.2808 - accuracy: 0.9001 - val_loss: 1.2857 - val_accuracy: 0.6301\n",
      "Epoch 83/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2805 - accuracy: 0.8993\n",
      "Epoch 83: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2807 - accuracy: 0.8990 - val_loss: 1.3046 - val_accuracy: 0.6096\n",
      "Epoch 84/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.8973\n",
      "Epoch 84: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2840 - accuracy: 0.8975 - val_loss: 1.3299 - val_accuracy: 0.6311\n",
      "Epoch 85/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2770 - accuracy: 0.8976\n",
      "Epoch 85: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 47ms/step - loss: 0.2770 - accuracy: 0.8976 - val_loss: 1.3164 - val_accuracy: 0.6130\n",
      "Epoch 86/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2716 - accuracy: 0.9029\n",
      "Epoch 86: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2721 - accuracy: 0.9027 - val_loss: 1.3551 - val_accuracy: 0.6089\n",
      "Epoch 87/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2678 - accuracy: 0.9029\n",
      "Epoch 87: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2681 - accuracy: 0.9028 - val_loss: 1.4206 - val_accuracy: 0.5751\n",
      "Epoch 88/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2746 - accuracy: 0.8996\n",
      "Epoch 88: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2746 - accuracy: 0.8996 - val_loss: 1.3296 - val_accuracy: 0.6339\n",
      "Epoch 89/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2751 - accuracy: 0.8992\n",
      "Epoch 89: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2751 - accuracy: 0.8992 - val_loss: 1.3365 - val_accuracy: 0.6127\n",
      "Epoch 90/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2743 - accuracy: 0.9023\n",
      "Epoch 90: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2751 - accuracy: 0.9020 - val_loss: 1.3840 - val_accuracy: 0.5881\n",
      "Epoch 91/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2708 - accuracy: 0.9020\n",
      "Epoch 91: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2708 - accuracy: 0.9020 - val_loss: 1.3450 - val_accuracy: 0.6270\n",
      "Epoch 92/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2739 - accuracy: 0.9004\n",
      "Epoch 92: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 47ms/step - loss: 0.2739 - accuracy: 0.9004 - val_loss: 1.3093 - val_accuracy: 0.5956\n",
      "Epoch 93/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2689 - accuracy: 0.9034\n",
      "Epoch 93: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2688 - accuracy: 0.9033 - val_loss: 1.3690 - val_accuracy: 0.5956\n",
      "Epoch 94/100\n",
      "365/366 [============================>.] - ETA: 0s - loss: 0.2692 - accuracy: 0.9038\n",
      "Epoch 94: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 47ms/step - loss: 0.2691 - accuracy: 0.9038 - val_loss: 1.3264 - val_accuracy: 0.6206\n",
      "Epoch 95/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2654 - accuracy: 0.9052\n",
      "Epoch 95: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2654 - accuracy: 0.9052 - val_loss: 1.3635 - val_accuracy: 0.6219\n",
      "Epoch 96/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2685 - accuracy: 0.9041\n",
      "Epoch 96: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 47ms/step - loss: 0.2685 - accuracy: 0.9041 - val_loss: 1.3733 - val_accuracy: 0.6130\n",
      "Epoch 97/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2704 - accuracy: 0.9060\n",
      "Epoch 97: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 47ms/step - loss: 0.2704 - accuracy: 0.9060 - val_loss: 1.3626 - val_accuracy: 0.5953\n",
      "Epoch 98/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2722 - accuracy: 0.9010\n",
      "Epoch 98: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2722 - accuracy: 0.9010 - val_loss: 1.3418 - val_accuracy: 0.6038\n",
      "Epoch 99/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2664 - accuracy: 0.9017\n",
      "Epoch 99: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2664 - accuracy: 0.9017 - val_loss: 1.3785 - val_accuracy: 0.5915\n",
      "Epoch 100/100\n",
      "366/366 [==============================] - ETA: 0s - loss: 0.2730 - accuracy: 0.9033\n",
      "Epoch 100: val_accuracy did not improve from 0.69092\n",
      "366/366 [==============================] - 17s 46ms/step - loss: 0.2730 - accuracy: 0.9033 - val_loss: 1.3500 - val_accuracy: 0.6305\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_words, 20, name=\"embedding_1\")) \n",
    "model.add(layers.LSTM(units=15, dropout=0.5, name=\"lstm_2\")) \n",
    "model.add(layers.Dense(3, activation='softmax', name=\"dense_3\"))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto', save_weights_only=False)\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test),callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "best_model = load_model('best_model1.hdf5')\n",
    "predictions = best_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['neutral', 'positive', 'negative']\n",
    "pred_labels = []\n",
    "\n",
    "for prediction in predictions: \n",
    "    pred_labels.append(labels[prediction.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:  Give your application an accessibility workout\n",
      "sentiment:  positive\n",
      "==========\n",
      "sentence:  Accerciser Accessibility Explorer\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  The default plugin layout for the bottom panel\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  The default plugin layout for the top panel\n",
      "sentiment:  neutral\n",
      "==========\n",
      "sentence:  A list of plugins that are disabled by default\n",
      "sentiment:  positive\n",
      "==========\n",
      "sentence:  Highlight duration\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  The duration of the highlight box when selecting accessible nodes\n",
      "sentiment:  neutral\n",
      "==========\n",
      "sentence:  Highlight border color\n",
      "sentiment:  neutral\n",
      "==========\n",
      "sentence:  The color and opacity of the highlight border.\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  Highlight fill color\n",
      "sentiment:  neutral\n",
      "==========\n",
      "sentence:  The color and opacity of the highlight fill.\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  API Browser\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  Browse the various methods of the current accessible\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  Hide private attributes\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  Method\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  Property\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  Value\n",
      "sentiment:  neutral\n",
      "==========\n",
      "sentence:  IPython Console\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  Interactive console for manipulating currently selected accessible\n",
      "sentiment:  neutral\n",
      "==========\n",
      "sentence:  Event monitor\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  _ Monitor Events\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  C _ lear Selection\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  Everything\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  Selected application\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  Selected accessible\n",
      "sentiment:  neutral\n",
      "==========\n",
      "sentence:  Source\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  Event Monitor\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  Shows events as they occur from selected types and sources\n",
      "sentiment:  positive\n",
      "==========\n",
      "sentence:  Highlight last event entry\n",
      "sentiment:  negative\n",
      "==========\n",
      "sentence:  Start / stop event recording\n",
      "sentiment:  neutral\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print(\"sentence: \", iitb_data[i])\n",
    "    print(\"sentiment: \", pred_labels[i])\n",
    "    print(\"=\"*10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
